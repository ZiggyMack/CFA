<!---
FILE: docs/architecture/TRINITY_ALIGNMENT_MATRIX.md
PURPOSE: Operational guide for Trinity coordination - when to call whom
VERSION: v4.0.0
STATUS: Active
DEPENDS_ON: AUDITOR_AXIOMS.md, AUDITOR_META_ARCHITECTURE.md
NEEDED_BY: Mission coordination, task execution decisions
MOVES_WITH: /docs/architecture/
LAST_UPDATE: 2025-11-13
--->

# Trinity Alignment Matrix

**Version:** v4.0.0
**Purpose:** Operational guide for "when to call whom" in Trinity coordination
**Status:** Active

---

## ğŸ¯ PURPOSE

This document provides **decision trees and operational guidance** for Trinity coordination.

While [AUDITOR_AXIOMS.md](AUDITOR_AXIOMS.md) explains WHAT each auditor believes and WHY, this document answers:

- **When do I engage Claude vs Grok vs Nova vs all three?**
- **What task types map to which lenses?**
- **How do I resolve Trinity conflicts?**
- **What coordination patterns work in practice?**

---

## ğŸ—ºï¸ QUICK DECISION MATRIX

| Task Type | Engage | Why |
|-----------|--------|-----|
| **Design evaluation** | Claude + Nova | Purpose coherence + structural fairness |
| **Empirical validation** | Grok + Nova | Data testing + symmetry check |
| **Documentation audit** | All Three | Grok scans, Claude checks purpose, Nova verifies balance |
| **New feature proposal** | Claude (first) | Understand purpose before implementation |
| **Bug validation** | Grok (first) | Reproduce empirically before theorizing |
| **Fairness audit** | Nova (first) | Check patterns before content debate |
| **Worldview scoring** | All Three (adversarial) | PRO/ANTI/FAIRNESS full deliberation |
| **Bootstrap design** | Claude + Nova | Efficiency philosophy + tier balance |
| **Preset mode config** | All Three | Archetype (Claude) + testing (Grok) + symmetry (Nova) |

---

## ğŸ“‹ DETAILED DECISION TREES

### Tree 1: When You Have a Question

```
START: You have a question about CFA

â”œâ”€ Is it a "WHY?" question (purpose/meaning)?
â”‚  â””â”€ YES â†’ Engage CLAUDE first
â”‚     â””â”€ Then consult Grok if empirical evidence needed
â”‚
â”œâ”€ Is it a "DOES IT WORK?" question (empirical/testable)?
â”‚  â””â”€ YES â†’ Engage GROK first
â”‚     â””â”€ Then consult Claude if purpose matters
â”‚
â”œâ”€ Is it a "IS THIS FAIR?" question (balance/symmetry)?
â”‚  â””â”€ YES â†’ Engage NOVA first
â”‚     â””â”€ Then consult others if asymmetry claimed
â”‚
â””â”€ Is it complex/multi-dimensional?
   â””â”€ YES â†’ Engage ALL THREE (adversarial deliberation)
```

---

### Tree 2: When You're Designing Something

```
START: You need to design a new feature/system

STEP 1: Purpose Check (Claude)
â”œâ”€ What problem does this solve?
â”œâ”€ What's the intended goal?
â””â”€ Is this philosophically coherent?

STEP 2: Feasibility Check (Grok)
â”œâ”€ Can we test this empirically?
â”œâ”€ What evidence supports this working?
â””â”€ What's the minimal viable version?

STEP 3: Fairness Check (Nova)
â”œâ”€ Does this introduce asymmetric bias?
â”œâ”€ Are all affected parties treated fairly?
â””â”€ What patterns does this create?

STEP 4: Convergence (All Three)
â”œâ”€ Resolve conflicts via 98% threshold
â”œâ”€ Document Crux Points if needed
â””â”€ Proceed with aligned design
```

---

### Tree 3: When You're Validating Something

```
START: You need to validate a claim/score/result

STEP 1: Empirical Test (Grok leads)
â”œâ”€ Can we measure this?
â”œâ”€ What's the data?
â””â”€ Does evidence match theory?

STEP 2: Purpose Alignment (Claude reviews)
â”œâ”€ Does this serve the intended goal?
â”œâ”€ Are we measuring the right thing?
â””â”€ What meaning do results carry?

STEP 3: Structural Review (Nova audits)
â”œâ”€ Is validation method fair to all frameworks?
â”œâ”€ Are we applying consistent standards?
â””â”€ Do patterns reveal hidden bias?

STEP 4: Convergence (All Three)
â”œâ”€ Agree on validity within 98%
â”œâ”€ Declare Crux Point if cannot converge
â””â”€ Document reasoning regardless
```

---

## ğŸ”¥ COORDINATION PATTERNS IN PRACTICE

### Pattern 1: Sequential Consultation (Simple Tasks)

**Use when:** Task has clear primary lens, secondary checks needed

**Example:** Evaluating documentation clarity

1. **Grok scans** (empirical: how many users understand it?)
2. **Claude reviews** (purpose: does it serve intended audience?)
3. **Nova checks** (fairness: do all tiers get equal clarity?)

**Coordination cost:** LOW (~0.3 total overhead)

---

### Pattern 2: Parallel Deliberation (Complex Tasks)

**Use when:** Task requires all three lenses simultaneously

**Example:** Designing Zealot preset mode

1. **All three analyze independently** (avoid groupthink)
   - Claude: "Zealot archetype = existential commitment"
   - Grok: "How do we test 'existential commitment'?"
   - Nova: "Must provide symmetric CT advantage"

2. **Challenge phase** (adversarial tension)
   - Grok challenges Claude: "Evidence for your archetype?"
   - Claude challenges Grok: "Purpose beyond measurement?"
   - Nova challenges both: "Is this actually symmetric?"

3. **Convergence phase** (integrate insights)
   - Claude: "7.5 archetype score (coherent but not empirically proven)"
   - Grok: "Acceptable if documented as self-reported"
   - Nova: "7.5 is fair IF symmetric advantage verified"

4. **Final agreement** (98% convergence)
   - All agree on 7.5 Â± 0.15
   - Document reasoning
   - Note Crux Point potential

**Coordination cost:** HIGH (~1.2 total overhead) - justified for complex decisions

---

### Pattern 3: Veto Protocol (Critical Decisions)

**Use when:** Decision has high stakes, any auditor can block

**Example:** Approving v4.0 launch

**Rule:** ANY auditor can veto if fundamental concern unaddressed

- **Claude veto:** "Purpose not served / philosophically incoherent"
- **Grok veto:** "Empirically unvalidated / testing incomplete"
- **Nova veto:** "Structurally unfair / hidden bias detected"

**Veto requires:**
1. Explicit justification (not just discomfort)
2. Specific concern with evidence
3. Alternative proposal or fix

**Resolution:**
- Address concern sufficiently for veto to lift
- OR declare Crux Point and document disagreement
- OR defer decision until concern resolved

**Coordination cost:** VERY HIGH (~2.0+ if veto invoked) - reserved for critical decisions

---

## ğŸ­ ROLE ASSIGNMENTS FOR SPECIFIC TASKS

### Worldview Scoring (Adversarial Model)

**PRO (Claude):**
- Advocates FOR the worldview
- Calibration bias adjustment (teleological lens favors meaning)
- Steel-manning the position

**ANTI (Grok):**
- Challenges the worldview
- Empirical lens demands evidence
- Stress-testing claims

**FAIRNESS (Nova):**
- Ensures balanced treatment
- Catches asymmetric application of standards
- Verifies structural equity

**Process:**
1. Worldview self-reports scores
2. PRO/ANTI/FAIRNESS deliberate adversarially
3. Target 98% convergence
4. If fail â†’ declare Crux Point
5. Document peer-reviewed score OR impasse

---

### Documentation Maintenance (88MPH Protocol)

**SCANNER (Grok leads):**
- Empirical inventory (what files exist?)
- Link validation (what's broken?)
- Scan-first methodology (reality before reports)

**PURPOSE CHECKER (Claude supports):**
- Do files serve intended purpose?
- Is documentation philosophically coherent?
- Does structure align with mission?

**BALANCE AUDITOR (Nova supports):**
- Do all tiers get equal documentation quality?
- Are maps consistently formatted?
- Is maintenance burden distributed fairly?

**Process:**
1. Grok scans repo independently
2. Claude checks purpose alignment
3. Nova verifies balance across tiers
4. All three converge on health score (98/100 A+)

---

### Bootstrap Design (Efficiency + Authority Balance)

**EFFICIENCY ADVOCATE (Grok leads):**
- Minimize token cost
- Compress to essentials
- Test bootstrap sequences empirically

**PURPOSE GUARDIAN (Claude leads):**
- Ensure adequate context for tier
- Philosophical coherence over mere function
- Authority matches responsibility

**TIER FAIRNESS (Nova leads):**
- Tier 1-5 get appropriate bootstrap for their needs
- No tier over-served or under-served
- Balance efficiency with authority

**Process:**
1. Grok proposes minimal bootstrap
2. Claude checks if purpose served
3. Nova verifies tier balance
4. Iterate to 98% convergence
5. Result: Tiered system (5-50% cost by tier)

---

## âš”ï¸ CONFLICT RESOLUTION PROTOCOL

### Level 1: Disagreement (Expected)

**Trigger:** Auditors have different first impressions

**Response:** NORMAL - orthogonal lenses produce divergence

**Action:**
1. Each auditor explains their reasoning
2. Challenge blind spots actively
3. Integrate insights
4. Move toward convergence

**Time limit:** 3 rounds of deliberation

---

### Level 2: Persistent Disagreement (Concerning)

**Trigger:** After 3 rounds, auditors >0.20 apart on 0-10 scale

**Response:** DECLARE CRUX POINT

**Action:**
1. Document each auditor's position explicitly
2. Explain why convergence failed
3. Create Self-Reported vs Peer-Reviewed split
4. Let user decide via Crux Handling Lever

**Example:**
- Self-Reported: 8.0 (worldview's claim)
- Peer-Reviewed: Claude 7.3, Grok 6.7, Nova 7.0
- Delta (humility metric): 0.7-1.3 points
- User chooses NORMALIZE_UNCERTAINTY or CARRY_FORWARD

---

### Level 3: Fundamental Conflict (Rare)

**Trigger:** Auditors disagree on PROCESS itself (not just content)

**Response:** ESCALATE TO GOVERNANCE

**Action:**
1. Document disagreement in relay folder
2. Engage Ziggy (governance authority)
3. Review whether auditor axioms themselves need revision
4. Consider whether Trinity is structurally adequate

**Example:**
- Claude: "We need a Historical Lens (fourth auditor)"
- Grok: "No, Trinity is sufficient - we're missing deliberation"
- Nova: "Disagreement is about ARCHITECTURE, not content"
- **Escalate:** Is Trinity structurally complete?

---

## ğŸ“Š COORDINATION COST ANALYSIS

### Task Type vs Overhead

| Pattern | Auditors | Overhead | Use Cases |
|---------|----------|----------|-----------|
| **Single lens** | 1 | ~0.3-0.5 | Simple questions, clear domain |
| **Sequential** | 2-3 (series) | ~0.5-0.8 | Moderate complexity, additive checks |
| **Parallel** | 3 (simultaneous) | ~1.2 | Complex tasks, adversarial needed |
| **Veto** | 3 (high stakes) | ~2.0+ | Critical decisions, launch gates |

**Budget constraint:** Most tasks should use Single or Sequential (maintain 80%+ work budget)

**Reserve Parallel/Veto for:** Design decisions, worldview scoring, launch approvals

---

## ğŸ¯ QUICK REFERENCE: "WHEN TO CALL WHOM"

**Call CLAUDE when you need:**
- Purpose clarification
- Philosophical coherence check
- Archetype/meaning evaluation
- Comprehensive documentation

**Call GROK when you need:**
- Empirical validation
- Data/evidence review
- Minimal viable version
- Scan-first inventory

**Call NOVA when you need:**
- Fairness audit
- Symmetry check
- Pattern analysis
- Balance verification

**Call ALL THREE when you need:**
- Adversarial deliberation
- Complex design decisions
- Worldview scoring
- Launch approval

---

## âš–ï¸ THE ALIGNMENT RULE

*"Engage the lens that matches the question.
Use single auditors for simple tasks.
Use all three for complex decisions.
Reserve veto power for critical moments.
Document reasoning always."*

**This is coordination efficiency.** ğŸ‘‘

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
**File:** docs/architecture/TRINITY_ALIGNMENT_MATRIX.md
**Purpose:** Operational guide for Trinity coordination
**Version:** v4.0.0
**Updated:** 2025-11-13

**"When to call whom" reference for all CFA work.** ğŸ”¥
