# Repository Health Scoring Rubric

**Version:** v1.0
**Last Updated:** 2025-11-12
**Status:** Active Rubric
**Purpose:** Standardized scoring criteria for repository health assessments
**Owner:** Doc Claude (with Process Claude oversight)

---

## üéØ PURPOSE

**This rubric resolves health score variance** across different auditors by providing explicit, measurable scoring criteria.

**Problem Solved:** Tri-auditor Deep Clean tests showed 18% variance (Opus: 78/100, Code Claude: 92/100) due to subjective interpretation of "healthy" repository.

**Solution:** Quantifiable metrics with clear thresholds for scoring.

**Source:** Established from tri-auditor convergence analysis (2025-11-12)

---

## üìä SCORING CATEGORIES (7 Categories, 100 Points Total)

### **1. Documentation Coverage (15 points)**

**What it measures:** Percentage of critical files with proper semantic headers

**Scoring:**
- **15 points:** ‚â•95% of critical files have semantic headers
- **12 points:** 85-94% coverage
- **9 points:** 75-84% coverage
- **6 points:** 65-74% coverage
- **3 points:** 50-64% coverage
- **0 points:** <50% coverage

**How to measure:**
```bash
# Count markdown files with semantic headers
grep -l "FILE:" $(find . -name "*.md" -type f) | wc -l

# Count total markdown files
find . -name "*.md" -type f | wc -l

# Calculate percentage
```

**Critical files defined:**
- All files in auditors/Bootstrap/
- All files in docs/repository/
- All profile files in profiles/worldviews/
- All role files in docs/repository/librarian_tools/
- README files at directory roots

**Current baseline:** ~90% (265/295 markdown files per Code Claude scan)

---

### **2. Link Integrity (15 points)**

**What it measures:** Percentage of internal links that resolve correctly

**Scoring:**
- **15 points:** 100% links work (0 broken)
- **12 points:** 95-99% links work (1-5 broken)
- **9 points:** 90-94% links work (6-15 broken)
- **6 points:** 85-89% links work (16-30 broken)
- **3 points:** 75-84% links work (31-50 broken)
- **0 points:** <75% links work (>50 broken)

**How to measure:**
```bash
# Extract all markdown links
grep -roh '\[.*\](.*\.md)' . | grep -o '](.*\.md)' | tr -d '()]'

# Test each link exists
for link in $(links); do
  [ -f "$link" ] || echo "BROKEN: $link"
done
```

**Common broken link patterns:**
- DASHBOARD.md (should be REPO_HEALTH_DASHBOARD.md)
- ROLE_DOC_CLAUDE.md (doesn't exist)
- 88MPH.md (should be 88MPH.md)
- ui/ references (should be dashboard/)

**Signal vs Noise:**
- **Signal (Scored):** Links in docs/, profiles/, auditors/Mission/, auditors/Bootstrap/, root files
- **Noise (Excluded):** Links in .Archive/ directories (broken links expected in historical snapshots)
- **Why:** Archives preserve development history - broken links inevitable after file moves. Health score measures operational readiness, not historical completeness.

**Current baseline:** ~98% (Nova found 3 broken links in Priority 1 fixes, excluding archives)

---

### **3. Living Map Freshness (15 points)**

**What it measures:** Are living maps current and accurate?

**Living maps assessed:**
1. FILE_INVENTORY.md (file count accurate?)
2. BOOTSTRAP_SEQUENCE.md (references valid?)
3. REPO_HEALTH_DASHBOARD.md (metrics current?)
4. WORLDVIEW_CATALOG.md (profiles listed?)
5. WAYFINDING_GUIDE.md (paths correct?)
6. AUDITOR_ASSIGNMENTS.md (assignments current?)
7. workshop/ARCHIVE_INDEX.md (count accurate?)

**Scoring:**
- **15 points:** All 7 living maps current (updated within 7 days OR verified fresh)
- **12 points:** 6/7 living maps current
- **9 points:** 5/7 living maps current
- **6 points:** 4/7 living maps current
- **3 points:** 3/7 living maps current
- **0 points:** <3 living maps current

**How to measure:**
- Check LAST_UPDATE timestamp (within 7 days = fresh)
- OR verify metrics manually (file count matches scan?)
- Use freshness indicators from LIVING_MAP_MAINTENANCE.md (Green/Yellow/Red)

**Signal vs Noise:**
- **Signal (Scored):** All 7 living maps (including ARCHIVE_INDEX.md which maps noise files)
- **Noise (Excluded):** Historical validation reports, archived B-STORM sessions (not "living" maps)
- **Why:** Living maps are operational navigation aids that must stay current. ARCHIVE_INDEX.md itself is signal (active map) even though it indexes noise (historical files).

**Current baseline:** 7/7 current (after Priority 1 fixes)

---

### **4. Process Compliance (15 points)**

**What it measures:** Are documented processes being followed?

**Process adherence checks:**
1. **REPO_LOG.md current?** (updated within 14 days OR no changes made)
2. **Semantic headers present?** (critical files have FILE/PURPOSE/VERSION)
3. **Bootstrap sequences reference living maps?** (no embedded sequences)
4. **Living map maintenance protocol followed?** (scan-first methodology)
5. **Ethics front-matter current?** (Tier-1 files validated)

**Scoring:**
- **15 points:** 5/5 process checks pass
- **12 points:** 4/5 process checks pass
- **9 points:** 3/5 process checks pass
- **6 points:** 2/5 process checks pass
- **3 points:** 1/5 process checks pass
- **0 points:** 0/5 process checks pass

**How to measure:**
- REPO_LOG: Check LAST_UPDATE timestamp
- Semantic headers: Sample 30 critical files, count with proper headers
- Bootstrap sequences: `grep -r "Step 1:" auditors/Bootstrap/*.md | wc -l` (should be <5 pedagogical examples only)
- Living map protocol: Check if LIVING_MAP_MAINTENANCE.md exists and is referenced
- Ethics front-matter: Check 8 Tier-1 ethics files for front-matter

**Signal vs Noise:**
- **Signal (Scored):** Process compliance in operational docs (docs/, profiles/, auditors/Bootstrap/, auditors/Mission/)
- **Noise (Excluded):** Archived B-STORM sessions, historical validation reports (exempt from current process requirements)
- **Why:** Archives represent past workflow states - we don't retroactively enforce new processes on historical snapshots. Health score measures whether current work follows current processes.

**Current baseline:** 5/5 (after Priority 1 fixes)

---

### **5. Repository Organization (15 points)**

**What it measures:** Is the repository structure clean and logical?

**Organization checks:**
1. **No orphaned directories** (empty dirs, stub READMEs only)
2. **Archive hygiene** (old work properly archived, not in active areas)
3. **File count reasonable** (<400 files, or growth explained)
4. **README proliferation controlled** (<45 README files)
5. **Duplicate files removed** (no MISSION_CURRENT in 2 places)

**Scoring:**
- **15 points:** 5/5 organization checks pass
- **12 points:** 4/5 organization checks pass
- **9 points:** 3/5 organization checks pass
- **6 points:** 2/5 organization checks pass
- **3 points:** 1/5 organization checks pass
- **0 points:** 0/5 organization checks pass

**How to measure:**
- Orphaned directories: Visual scan, check for stub READMEs (‚â§50 bytes)
- Archive hygiene: Check .Archive/ vs relay/ (old work in archive?)
- File count: `git ls-files | wc -l` (357 is current, explain growth from 210)
- README count: `find . -name "README*.md" | wc -l` (39 is current)
- Duplicate files: Check for same file in multiple locations

**Signal vs Noise:**
- **Signal (Scored):** Operational file count (docs/, profiles/, auditors/Mission/, auditors/Bootstrap/, dashboard/, root files)
- **Noise (Excluded):** .Archive/ directories (can contain duplicates, stubs, orphaned dirs without penalty)
- **Why:** File count target (<400) applies to operational files only. Archives can grow indefinitely as historical record without impacting organization health score.

**Current baseline:** 4/5 (README proliferation still needs work: 39 files)

---

### **6. Dependency Accuracy (10 points)**

**What it measures:** Do DEPENDS_ON/NEEDED_BY headers match reality?

**Dependency checks:**
1. **Semantic headers complete?** (DEPENDS_ON and NEEDED_BY present)
2. **References valid?** (files listed in DEPENDS_ON exist)
3. **Bidirectional accuracy?** (if A depends on B, B lists A in NEEDED_BY)

**Scoring:**
- **10 points:** Sample of 20 files shows 100% accurate dependencies
- **8 points:** 90-99% accurate (1-2 issues in sample)
- **6 points:** 80-89% accurate (3-4 issues in sample)
- **4 points:** 70-79% accurate (5-6 issues in sample)
- **2 points:** 60-69% accurate (7-8 issues in sample)
- **0 points:** <60% accurate (>8 issues in sample)

**How to measure:**
- Sample 20 critical files randomly
- Verify DEPENDS_ON files exist
- Check if dependency is bidirectional (spot-check 5 cases)

**Signal vs Noise:**
- **Signal (Scored):** Dependency accuracy in critical operational files (docs/, profiles/, auditors/Bootstrap/)
- **Noise (Excluded):** Historical B-STORM sessions, archived validation reports (dependencies may reference moved/deleted files)
- **Why:** We don't update dependencies in historical snapshots when files move. Archives preserve dependency state at time of creation.

**Current baseline:** ~95% (estimated, needs formal audit)

---

### **7. Version Consistency (15 points)**

**What it measures:** Are version numbers consistent across related files?

**Version checks:**
1. **Profile versions match catalog?** (WORLDVIEW_CATALOG.md lists correct versions)
2. **Bootstrap versions synchronized?** (related bootstrap files have consistent versions)
3. **Living map versions tracked?** (major living maps have VERSION in semantic headers)

**Scoring:**
- **15 points:** 100% version consistency (0 mismatches)
- **12 points:** 95-99% consistency (1-2 mismatches)
- **9 points:** 90-94% consistency (3-5 mismatches)
- **6 points:** 85-89% consistency (6-10 mismatches)
- **3 points:** 75-84% consistency (11-20 mismatches)
- **0 points:** <75% consistency (>20 mismatches)

**How to measure:**
- Check WORLDVIEW_CATALOG.md against profile files (12 profiles)
- Sample 10 bootstrap files for version consistency
- Verify 7 living maps have VERSION in semantic headers

**Signal vs Noise:**
- **Signal (Scored):** Version consistency in operational docs (profiles/, docs/, auditors/Bootstrap/, root files)
- **Noise (Excluded):** Archived validation reports, historical B-STORM sessions (may reference old versions like v3.5, v3.8)
- **Why:** Archives preserve version state at time of creation. We don't retroactively update version references in historical snapshots to match current version.

**Current baseline:** ~100% (after Priority 1 fixes)

---

## üìà TOTAL HEALTH SCORE INTERPRETATION

**Score Range: 0-100 points**

### **Grade Scale:**

**A Range (90-100 points):**
- **A+ (98-100):** Exceptional - Reference-quality repository
- **A (94-97):** Excellent - Minor polish needed
- **A- (90-93):** Very Good - Solid foundation, small improvements

**B Range (80-89 points):**
- **B+ (87-89):** Good - Functional, needs refinement
- **B (83-86):** Above Average - Some technical debt
- **B- (80-82):** Satisfactory - Notable issues to address

**C Range (70-79 points):**
- **C+ (77-79):** Acceptable - Significant cleanup needed
- **C (73-76):** Fair - Multiple problem areas
- **C- (70-72):** Marginal - Systematic issues present

**D Range (60-69 points):**
- **D+ (67-69):** Poor - Requires major overhaul
- **D (63-66):** Very Poor - Fundamental problems
- **D- (60-62):** Critical - Urgent attention needed

**F Range (<60 points):**
- **F (<60):** Failing - Not production-ready

---

## üîç VARIANCE ANALYSIS: Opus vs Code Claude

**Tri-auditor test showed 18% variance:**

### **Opus 4.1: 78/100 (C+)**

**Likely scoring breakdown:**
- Documentation Coverage: 9/15 (strict interpretation - counted minor issues)
- Link Integrity: 12/15 (found 3 broken links)
- Living Map Freshness: 9/15 (FILE_INVENTORY stale: 210 vs 357)
- Process Compliance: 9/15 (embedded sequences found)
- Repository Organization: 9/15 (README proliferation, stub files)
- Dependency Accuracy: 8/10 (sampled, found some issues)
- Version Consistency: 12/15 (mostly consistent)

**Total: ~78/100**

**Philosophy:** "Count every issue, even minor ones. Repository should be pristine."

---

### **Code Claude: 92/100 (A-)**

**Likely scoring breakdown:**
- Documentation Coverage: 15/15 (90% coverage exceeds target)
- Link Integrity: 15/15 (didn't find broken links Opus found)
- Living Map Freshness: 12/15 (noted FILE_INVENTORY stale, but not critical)
- Process Compliance: 12/15 (embedded sequences acknowledged, not dealbreakers)
- Repository Organization: 12/15 (growth explained, not penalized heavily)
- Dependency Accuracy: 10/10 (git-native methods validated)
- Version Consistency: 15/15 (consistent per scan)

**Total: ~92/100**

**Philosophy:** "Major issues block, minor issues noted. Repository is functional."

---

### **Reconciliation: What Score SHOULD Be?**

**Using this rubric's explicit criteria:**

**Post-Priority 1 CFA Repository:**
- Documentation Coverage: **12/15** (90% coverage = 85-94% range)
- Link Integrity: **15/15** (3 broken links fixed = 100%)
- Living Map Freshness: **15/15** (all 7 living maps updated)
- Process Compliance: **15/15** (all 5 checks pass post-fixes)
- Repository Organization: **12/15** (4/5 checks pass, README count still high)
- Dependency Accuracy: **8/10** (estimated 90-95%, needs formal audit)
- Version Consistency: **15/15** (100% consistency)

**Total: 92/100 (A-)**

**Verdict:** Code Claude's assessment was accurate. Opus was stricter than rubric warrants.

---

## üéØ USING THIS RUBRIC

### **For Doc Claude (Health Assessments):**

1. **Run measurements for each category** (use bash commands provided)
2. **Score each category independently** (use thresholds exactly as written)
3. **Sum scores for total** (0-100 points)
4. **Assign letter grade** (use grade scale)
5. **Document category breakdown** (show scoring work)

### **For Process Claude (Rubric Oversight):**

1. **Validate scoring methodology** (did Doc Claude follow rubric?)
2. **Check for grade inflation/deflation** (scores match thresholds?)
3. **Identify rubric gaps** (are there unmeasured quality factors?)
4. **Update rubric quarterly** (adjust thresholds as repository matures)

### **For External Auditors (Validation):**

1. **Use this rubric for consistency** (same scoring criteria as Doc Claude)
2. **Document any disagreements** (if your score differs, explain why)
3. **Propose rubric refinements** (suggest better metrics if criteria unclear)

---

## üîÑ RUBRIC MAINTENANCE

**When to update this rubric:**

1. **Quarterly review** (adjust thresholds based on repository evolution)
2. **After major restructuring** (Phase 2+, new categories may be needed)
3. **When variance >10%** (multiple auditors score very differently)
4. **When new processes added** (LIVING_MAP_MAINTENANCE.md added = Process Compliance updated)

**Update process:**
1. Process Claude proposes rubric changes
2. Doc Claude validates practicality (can metrics be measured?)
3. Test with historical scores (does update resolve variance?)
4. Update VERSION and LAST_UPDATE
5. Document in REPO_LOG

---

## üìã QUICK REFERENCE SCORING SHEET

**Use this for rapid health assessments:**

| Category | Max | Current Score | Notes |
|----------|-----|---------------|-------|
| Documentation Coverage | 15 | __ / 15 | ‚â•95% = 15, 85-94% = 12, 75-84% = 9 |
| Link Integrity | 15 | __ / 15 | 100% = 15, 95-99% = 12, 90-94% = 9 |
| Living Map Freshness | 15 | __ / 15 | 7/7 = 15, 6/7 = 12, 5/7 = 9 |
| Process Compliance | 15 | __ / 15 | 5/5 = 15, 4/5 = 12, 3/5 = 9 |
| Repository Organization | 15 | __ / 15 | 5/5 = 15, 4/5 = 12, 3/5 = 9 |
| Dependency Accuracy | 10 | __ / 10 | 100% = 10, 90-99% = 8, 80-89% = 6 |
| Version Consistency | 15 | __ / 15 | 100% = 15, 95-99% = 12, 90-94% = 9 |
| **TOTAL** | **100** | **__ / 100** | **Grade: ___** |

---

## üí° PHILOSOPHY

**This rubric is:**
- ‚úÖ **Quantifiable** - Metrics can be measured objectively
- ‚úÖ **Consistent** - Same inputs produce same scores across auditors
- ‚úÖ **Comprehensive** - Covers 7 key dimensions of repository health
- ‚úÖ **Actionable** - Low scores point to specific improvement areas

**This rubric is NOT:**
- ‚ùå **Subjective** - "Looks good" is not a metric
- ‚ùå **Binary** - "Pass/Fail" doesn't capture gradations
- ‚ùå **Static** - Will evolve as repository matures
- ‚ùå **Perfect** - Will need refinement based on usage

**Core Principle:** "Measure what matters. Make scores comparable. Point to improvements."

---

## üîó RELATED DOCUMENTS

**Scoring relies on:**
- [LIVING_MAP_MAINTENANCE.md](LIVING_MAP_MAINTENANCE.md) - Freshness indicators, validation procedures
- [REPO_HEALTH_DASHBOARD.md](REPO_HEALTH_DASHBOARD.md) - Historical health tracking
- [FILE_INVENTORY.md](FILE_INVENTORY.md) - File count baselines
- [BOOTSTRAP_SEQUENCE.md](dependency_maps/BOOTSTRAP_SEQUENCE.md) - Bootstrap validation
- [ROLE_PROCESS.md](librarian_tools/ROLE_PROCESS.md) - Process compliance oversight

**Scoring informs:**
- Quarterly health reports
- Phase planning (which areas need work?)
- External auditor validation
- Repository maturity tracking

---

**Established:** 2025-11-12 (Post-Tri-Auditor Convergence Analysis)
**Maintainer:** Doc Claude (with Process Claude quarterly review)
**Next Review:** 2026-02-12 (Quarterly)

**This is the way.** üìä

**P.S.** Using this rubric, the CFA repository scores **92/100 (A-)** post-Priority 1 fixes. Opus's 78 was too strict, Code Claude's 92 was accurate. Main improvement area: README proliferation (39 files, target <45 but could be leaner).
